{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c8c20eb-6eb2-4671-81cd-7f79b2626336",
   "metadata": {},
   "source": [
    "# 04.3- Python + LLMs\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>You will need to setup an OpenAI key to fully use this notebook.</b>\n",
    "    <b>Instructions will be sent via email.</b>\n",
    "</div>\n",
    "\n",
    "Using an LLM on a prompt? By now you have most likely have done that.  \n",
    "Copy pasting inputs and outputs and whatnot. Like if it is still 2023.  \n",
    "In this notebook we'll introduce how to get a working python interaction with an LLM that you can later include in your projects.  \n",
    "First, we need some brief definitions:\n",
    "\n",
    "## Intro to Large Language Models (in a programatic sense)\n",
    "\n",
    "### LLMs: Large Language Models\n",
    "A Large Language Model is a type of artificial intelligence model trained on vast amounts of text data to understand and generate human-like language. Think about our Shakespeare example, but on an a much much larger scale. These models are based on deep learning techniques, specifically neural networks, and are capable of performing various natural language processing tasks.\n",
    "\n",
    "### Generative Models\n",
    "While LLMs handle language, other models may generate art, recognize sound (Speech-To-Text), create videos, etc.. For coding purposes, one usually uses an LLM optimized for code, for example. To generate images you might use DALL-E and so on.\n",
    "\n",
    "### Agents\n",
    "Agents are software systems that have a [model at its core](https://developer.nvidia.com/blog/introduction-to-llm-agents/). The agents add memory to the model, making the model \"aware\" of the history of a chat you are having. The model can be any kind of generative model (images, text, etc.). An agent might even use several AI models under the hood, making it multi-task.\n",
    "\n",
    "### Langchain\n",
    "[Langchain](https://en.wikipedia.org/wiki/LangChain) is a software framework designed to make your interaction with AI Models and Agents much easier. It is still in its early stages, so we will use it with caution.\n",
    "\n",
    "### In this course\n",
    "We are going to focus on LLMs suited for coding and data analysis. Please make sure you have the secret key configured for OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3d9d5fb-87a4-4346-8d4d-af968ff0aaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Using cached langchain-0.1.8-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Using cached SQLAlchemy-2.0.27-cp311-cp311-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Using cached aiohttp-3.9.3-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Using cached dataclasses_json-0.6.4-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.21 (from langchain)\n",
      "  Using cached langchain_community-0.0.21-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting langchain-core<0.2,>=0.1.24 (from langchain)\n",
      "  Using cached langchain_core-0.1.24-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.0 (from langchain)\n",
      "  Using cached langsmith-0.1.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy<2,>=1 (from langchain)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "Collecting pydantic<3,>=1 (from langchain)\n",
      "  Using cached pydantic-2.6.1-py3-none-any.whl.metadata (83 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain)\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached frozenlist-1.4.1-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached multidict-6.0.5-cp311-cp311-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached yarl-1.9.4-cp311-cp311-win_amd64.whl.metadata (32 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Using cached marshmallow-3.20.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from langchain-core<0.2,>=0.1.24->langchain) (4.3.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from langchain-core<0.2,>=0.1.24->langchain) (23.2)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain)\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.2 (from pydantic<3,>=1->langchain)\n",
      "  Using cached pydantic_core-2.16.2-cp311-none-win_amd64.whl.metadata (6.6 kB)\n",
      "Collecting typing-extensions>=4.6.1 (from pydantic<3,>=1->langchain)\n",
      "  Using cached typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Using cached greenlet-3.0.3-cp311-cp311-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.24->langchain) (1.3.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Using cached langchain-0.1.8-py3-none-any.whl (816 kB)\n",
      "Using cached aiohttp-3.9.3-cp311-cp311-win_amd64.whl (365 kB)\n",
      "Using cached dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langchain_community-0.0.21-py3-none-any.whl (1.7 MB)\n",
      "Using cached langchain_core-0.1.24-py3-none-any.whl (241 kB)\n",
      "Using cached langsmith-0.1.3-py3-none-any.whl (60 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "Using cached pydantic-2.6.1-py3-none-any.whl (394 kB)\n",
      "Using cached pydantic_core-2.16.2-cp311-none-win_amd64.whl (1.9 MB)\n",
      "Using cached SQLAlchemy-2.0.27-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached frozenlist-1.4.1-cp311-cp311-win_amd64.whl (50 kB)\n",
      "Using cached greenlet-3.0.3-cp311-cp311-win_amd64.whl (292 kB)\n",
      "Using cached marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
      "Using cached multidict-6.0.5-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Using cached typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached yarl-1.9.4-cp311-cp311-win_amd64.whl (76 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: typing-extensions, tenacity, numpy, mypy-extensions, multidict, marshmallow, jsonpatch, greenlet, frozenlist, annotated-types, yarl, typing-inspect, SQLAlchemy, pydantic-core, aiosignal, pydantic, dataclasses-json, aiohttp, langsmith, langchain-core, langchain-community, langchain\n",
      "Successfully installed SQLAlchemy-2.0.27 aiohttp-3.9.3 aiosignal-1.3.1 annotated-types-0.6.0 dataclasses-json-0.6.4 frozenlist-1.4.1 greenlet-3.0.3 jsonpatch-1.33 langchain-0.1.8 langchain-community-0.0.21 langchain-core-0.1.24 langsmith-0.1.3 marshmallow-3.20.2 multidict-6.0.5 mypy-extensions-1.0.0 numpy-1.26.4 pydantic-2.6.1 pydantic-core-2.16.2 tenacity-8.2.3 typing-extensions-4.9.0 typing-inspect-0.9.0 yarl-1.9.4\n",
      "Collecting langchain-openai\n",
      "  Using cached langchain_openai-0.0.6-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.16 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from langchain-openai) (0.1.24)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from langchain-openai) (1.26.4)\n",
      "Collecting openai<2.0.0,>=1.10.0 (from langchain-openai)\n",
      "  Using cached openai-1.12.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tiktoken<1,>=0.5.2 (from langchain-openai)\n",
      "  Using cached tiktoken-0.6.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (6.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (4.3.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (0.1.3)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (2.6.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (8.2.3)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.10.0->langchain-openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (0.26.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.3.0)\n",
      "Collecting tqdm>4 (from openai<2.0.0,>=1.10.0->langchain-openai)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.9.0)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1,>=0.5.2->langchain-openai)\n",
      "  Using cached regex-2023.12.25-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain-openai) (3.6)\n",
      "Requirement already satisfied: certifi in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (1.0.3)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1.16->langchain-openai) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1.16->langchain-openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1.16->langchain-openai) (2.16.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1.16->langchain-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1.16->langchain-openai) (2.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\luisg\\anaconda3\\envs\\llm\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.10.0->langchain-openai) (0.4.6)\n",
      "Using cached langchain_openai-0.0.6-py3-none-any.whl (29 kB)\n",
      "Using cached openai-1.12.0-py3-none-any.whl (226 kB)\n",
      "Using cached tiktoken-0.6.0-cp311-cp311-win_amd64.whl (798 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached regex-2023.12.25-cp311-cp311-win_amd64.whl (269 kB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, distro, tiktoken, openai, langchain-openai\n",
      "Successfully installed distro-1.9.0 langchain-openai-0.0.6 openai-1.12.0 regex-2023.12.25 tiktoken-0.6.0 tqdm-4.66.2\n"
     ]
    }
   ],
   "source": [
    "# Uncomment and run, or copy paste to terminal and run\n",
    "!pip install langchain\n",
    "!pip install -U langchain-openai\n",
    "#You might need to restart the kernel after installing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b27963a-e8c8-48d4-9d5a-9bb5613c0de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use a model from the OpenAI family\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4491d89b-eddf-4a5b-ae78-b27444874268",
   "metadata": {},
   "source": [
    "### Using models with Langchain\n",
    "\n",
    "The `langchain` framework has dedicated modules for specific models. For example, if a model ws developed by OpenAI, there is a dedicated `langchain-openai` set of tools.  \n",
    "This is all still pretty new, as you can see by the version numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aa2c95f-0e43-4674-bf06-2a239dec6d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.8'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bb24a0-835b-4cfb-9603-e728485cb769",
   "metadata": {},
   "source": [
    "In this course we are going to use OpenAI models for three reasons:\n",
    "1. Maturity of the integration between OpeanAI and Langchain\n",
    "2. Quality/Price of the standard model used: gpt-3.5-turbo is cheap and provides adequate results for coding/data analysis.\n",
    "3. Results are more in line with ChatGPT, which is what students may be more familiar with (same family of models)\n",
    "\n",
    "Using the model requires payment, but it is the only paid component of this course. There are open source models that you can deploy on your machine if you are developing a start-up, for example.\n",
    "\n",
    "More resources:\n",
    "* https://github.com/Hannibal046/Awesome-LLM\n",
    "* https://github.com/langchain-ai/langchain/tree/master/cookbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1a89f4-c042-4ae7-8c28-c0041d15c7ff",
   "metadata": {},
   "source": [
    "---\n",
    "### Let's use a model\n",
    "\n",
    "If there is one thing you should take from this course is that Documentation exists for a reason.  \n",
    "Take a look at the OpenAI documentation for models: https://platform.openai.com/docs/models\n",
    "\n",
    "#### Written by chatgpt: (in this course we will focus only on the first four)\n",
    "When using a large language model like GPT-3, there are several key parameters that you can adjust to customize the behavior of the model for your specific task or application. Here are some of the main parameters:\n",
    "* Model Size: The size of the model refers to the number of parameters it has. Larger models typically have more capacity to understand and generate complex language, but they also require more computational resources.\n",
    "* Context Length: This parameter determines the amount of context the model considers when generating responses. A longer context allows the model to better understand the input, but it also increases computational requirements.\n",
    "* Temperature: Temperature is a parameter that controls the randomness of the model's output. Higher temperature values (e.g., 0.8) lead to more creative and diverse responses, while lower values (e.g., 0.2) make the output more deterministic and focused.\n",
    "* Max Tokens: This parameter limits the length of the generated output. It can be useful to prevent overly long responses.\n",
    "* Top-k and Top-p/Nucleus Sampling: These parameters control the diversity of the generated output. Top-k sampling selects the top k most likely next tokens, while top-p (nucleus) sampling selects tokens until the cumulative probability exceeds a certain threshold p.\n",
    "* Frequency Penalties: You can apply penalties or bonuses based on the frequency of tokens in the output. This can be used to encourage or discourage the use of specific words or phrases.\n",
    "* Prompt Engineering: Crafting a well-defined and clear prompt can significantly impact the quality and relevance of the model's responses. Experimenting with different prompts can help achieve the desired output.\n",
    "* Fine-tuning: If you have specific requirements for your task, you can fine-tune the model on a custom dataset. Fine-tuning allows the model to learn task-specific patterns and improve performance.\n",
    "* Resource Allocation: Depending on the hardware you are using, you may need to adjust parameters to fit within the available resources. Larger models and longer contexts require more memory and processing power.\n",
    "* API Usage: When using a language model through an API, there might be additional parameters or constraints specific to the API. Understanding the API documentation is crucial for effective utilization.\n",
    "\n",
    "It's essential to experiment with these parameters to find the right balance for your specific use case. Keep in mind that tweaking parameters may require some trial and error to achieve the desired results.\n",
    "\n",
    "---\n",
    "\n",
    "Since we are going to do introductory level requests, and are looking mostly for coding solutions, we will keep the temperature low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b08063c-da5b-4f5e-96ff-e7833ebfc97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.1) #Check shift+Tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0079c7f5-1dbf-4f97-a98e-ecad814dae90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-3.5-turbo'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Play around with the autocomplete. Check what methods and attributes our instance \"llm\" has.\n",
    "llm.model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7eade2-979a-4d91-b327-55a241994c30",
   "metadata": {},
   "source": [
    "#### The most basic interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8c2837e-7d83-4767-8c91-9d87ba314338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Optimus Prime was originally going to be named \"Orion Pax\" in the Transformers series, but the name was changed to Optimus Prime before the character was introduced.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('Here is a fun fact about Optimus Prime:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75dfe150-74b7-4f26-ad06-1450f407d31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke('Which music album is considered the best of all time by critics?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c43fda83-4178-4b26-8945-0bdd8f378734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There is no definitive answer to this question as opinions on the best music album of all time vary among critics. However, some albums that are often cited as contenders for the title of best album of all time include:\\n\\n- The Beatles - \"Sgt. Pepper\\'s Lonely Hearts Club Band\"\\n- Pink Floyd - \"The Dark Side of the Moon\"\\n- Bob Dylan - \"Highway 61 Revisited\"\\n- The Beach Boys - \"Pet Sounds\"\\n- Michael Jackson - \"Thriller\"\\n\\nUltimately, the best music album of all time is subjective and depends on individual taste and preferences.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1f73bf-5311-47f4-b932-4ae7b611a205",
   "metadata": {},
   "source": [
    "#### For better printing on a Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "716e3c1a-06a4-43cb-9d4e-00372d10b99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2ca0737-b920-45b1-b985-bda812726320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "There is no definitive answer to this question as opinions on the best music album of all time vary among critics. However, some albums that are often cited as contenders for the title of best album of all time include:\n",
       "\n",
       "- The Beatles - \"Sgt. Pepper's Lonely Hearts Club Band\"\n",
       "- Pink Floyd - \"The Dark Side of the Moon\"\n",
       "- Bob Dylan - \"Highway 61 Revisited\"\n",
       "- The Beach Boys - \"Pet Sounds\"\n",
       "- Michael Jackson - \"Thriller\"\n",
       "\n",
       "Ultimately, the best music album of all time is subjective and depends on individual taste and preferences."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d71b2e4-b47f-48fd-bffc-2eaba662d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke('Please remove albums released later than 1990.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b59da00b-0dd1-46db-a101-c8267a766633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'm sorry, but I cannot provide a list of albums released before 1990 as I am unable to access specific information about album release dates. If you have any other requests or questions, feel free to ask."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0419838a-66e2-464d-8e7d-bb4a312d2022",
   "metadata": {},
   "source": [
    "The LLM has no recollection of your previous message. We are doing \"Zero-Shot calls\" to the model, meaning there is no notion of state. In other words, the instance you started has no memory of past prompts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
